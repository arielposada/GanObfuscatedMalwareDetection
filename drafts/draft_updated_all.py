from fastai.vision.all import *
from fastai.vision.gan import *
import fastai.torch_core
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.checkpoint import checkpoint
import argparse
import os
import math
import torchvision.transforms as transforms
from torchvision.utils import save_image
from torch.utils.data import DataLoader
from torchvision import datasets
from torch.autograd import Variable
import torch.nn.functional as F
import random
import torch.nn.parallel
import torch.backends.cudnn as cudnn
import torch.optim as optim
import torch.utils.data
import torchvision.datasets as dset
import torchvision.utils as vutils
import numpy as np
from torch.cuda.amp import GradScaler, autocast
import datetime
import threading
import time
import timm

bs, size = 8, 224
img_size = 224
path = "/home/ariel.posada/AFGIE1/NonObfuscatedImg"
z_dim = 100  
num_classes = 2  # 'Goodware' y 'Malware'
labels = ('Goodware', 'Malware')

def get_dls(bs, size):
    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),
                       get_items=get_image_files,
                       splitter=RandomSplitter(valid_pct=0.2, seed=42),
                       get_y=parent_label,
                       item_tfms=Resize(size))
    dls = dblock.dataloaders(path, bs=bs)
    dls.c = 3
    return dls


from torchvision.models import resnet18

class ConditionalDiscriminator(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        self.resnet = resnet18(pretrained=True)
        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 1)  # Cambiar para salida binaria

        # Capa adicional para manejar las etiquetas de clase
        self.class_embedding = nn.Embedding(num_classes, 512)  # 512 es la característica antes de la capa fc en ResNet18

    def forward(self, x, labels):
        x = self.resnet.conv1(x)
        x = self.resnet.bn1(x)
        x = self.resnet.relu(x)
        x = self.resnet.maxpool(x)

        x = self.resnet.layer1(x)
        x = self.resnet.layer2(x)
        x = self.resnet.layer3(x)
        x = self.resnet.layer4(x)

        x = self.resnet.avgpool(x)
        x = torch.flatten(x, 1)

        # Combinar las características con las etiquetas
        class_embedding = self.class_embedding(labels)
        x = x + class_embedding

        x = self.resnet.fc(x)
        return torch.sigmoid(x)

# Instanciar el discriminador
netD = ConditionalDiscriminator(num_classes=num_classes).cuda()


class ConditionalGenerator(nn.Module):
    def __init__(self, z_dim=100, img_size=224, n_channels=1, num_classes=2):
        super().__init__()
        self.label_emb = nn.Embedding(num_classes, z_dim)

        self.gen = nn.Sequential(
            self._block(z_dim * 2, 128, stride=1, padding=0),  # Se duplica el tamaño para incluir la incrustación de clase
            self._block(128, 256),
            self._block(256, 512),
            self._block(512, 1024),
            nn.ConvTranspose2d(1024, n_channels, kernel_size=4, stride=2, padding=1),
            nn.Tanh()
        )

    def _block(self, in_channels, out_channels, kernel_size=4, stride=2, padding=1):
        return nn.Sequential(
            nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(True)
        )

    def forward(self, z, labels):
        # Incrustar etiquetas y combinarlas con z
        c = self.label_emb(labels)
        x = torch.cat([z, c], 1)
        return self.gen(x)


class ConditionalGANLearner(Learner):
    def __init__(self, dls, generator, discriminator, loss_func, opt_func=Adam):
        super().__init__(dls, generator, loss_func=loss_func, opt_func=opt_func)
        self.discriminator = discriminator

    def one_batch(self, i, b):
        real, labels = b
        noise = torch.randn(real.size(0), z_dim, 1, 1, device=real.device)
        fake = self.model(noise, labels)
        self.pred = fake
        self.loss = self.loss_func(self.discriminator, fake, labels)

def disc_loss_func(pred, target):
    return F.binary_cross_entropy(pred, target)

class GANLearner(Learner):
    def __init__(self, dls, generator, discriminator, gen_loss_func, disc_loss_func, opt_func=Adam):
        super().__init__(dls, generator, loss_func=gen_loss_func, opt_func=opt_func)
        self.discriminator = discriminator
        self.disc_loss_func = disc_loss_func
        self.gen_loss_func = gen_loss_func

    def one_batch(self, i, b):
        real, labels = b
        noise = torch.randn(real.size(0), z_dim, 1, 1, device=real.device)

        # Entrenar Discriminador
        self.discriminator.zero_grad()
        pred_real = self.discriminator(real, labels)
        lossD_real = self.disc_loss_func(pred_real, torch.ones_like(pred_real))
        lossD_real.backward()

        fake = self.model(noise, labels).detach()
        pred_fake = self.discriminator(fake, labels)
        lossD_fake = self.disc_loss_func(pred_fake, torch.zeros_like(pred_fake))
        lossD_fake.backward()

        self.d_optimizer.step()  # Asegúrate de definir y asignar d_optimizer

        # Entrenar Generador
        self.model.zero_grad()
        fake = self.model(noise, labels)
        pred_fake = self.discriminator(fake, labels)
        self.loss = self.gen_loss_func(pred_fake, torch.ones_like(pred_fake))
        self.loss.backward()
        self.opt.step()

# Funciones de pérdida
def disc_loss_func(pred, target):
    return F.binary_cross_entropy(pred, target)

# Crear el Learner
learner = GANLearner(dls, netG, netD, conditional_generator_loss, disc_loss_func)



def conditional_generator_loss(discriminator, fake, labels):
    # Crea etiquetas verdaderas (1s) para calcular la pérdida
    valid = torch.ones(fake.size(0), 1, device=fake.device)
    
    # Calcula la pérdida del generador
    # El generador tiene éxito si el discriminador clasifica las imágenes falsas como reales
    predictions = discriminator(fake, labels)
    loss = F.binary_cross_entropy(predictions, valid)
    return loss

dls = get_dls()

# Instanciar el generador condicional
netG = ConditionalGenerator(z_dim=z_dim, img_size=img_size, n_channels=1, num_classes=num_classes).cuda()

learner = GANLearner(dls, netG, netD, conditional_generator_loss, disc_loss_func)

learner.fit_one_cycle(20, 0.0001)