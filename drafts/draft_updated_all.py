from fastai.vision import *
from fastai.vision.all import *
from fastai.vision.gan import *
from fastai.metrics import accuracy, Precision, Recall, F1Score
import datetime
from torchvision.utils import save_image
from pathlib import Path
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import numpy as np
import sys
import torch
import torch.nn as nn
import torch.nn.functional as F

# Se usan los parámetros originales, 64, 224
#bs, size = 64, 224
bs,size=32, 224
# bs,size = 24,160
#bs,size = 8,256
arch = resnet34

path = "/home/ariel.posada/AFGIE1/NonObfuscatedImg"

def get_dls(bs,size):
    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),
      get_items=get_image_files,
      splitter=RandomSplitter(valid_pct=0.2, seed=42),
      get_y=parent_label,
                       item_tfms=Resize(size))
      #item_tfms=Resize(224))
    #dls = dblock.dataloaders(path, bs=64)
    dls = dblock.dataloaders(path, bs=bs)
    dls.c = 3
    return dls

dls_gen = get_dls(bs,size)

wd = 1e-3
#loss_gen = MSELossFlat()
loss_gen = AdaptiveLoss(nn.BCEWithLogitsLoss())
y_range = ('Goodware','Malware')


# Establecer el límite para el número de líneas de traceback a None para mostrar todo el traceback
sys.tracebacklimit = None

# Verificar si CUDA está disponible y asignar el dispositivo correspondiente
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Función de activación LeakyReLU
def leaky_relu():
    return nn.LeakyReLU(0.2, inplace=True)

# Clase para definir el crítico personalizado de la GAN
class CustomCritic(nn.Module):
    def __init__(self, in_size, n_channels, n_features=64, n_extra_layers=0):
        super().__init__()
        self.conv_layers = nn.Sequential(
            ConvLayer(n_channels, n_features, 4, 2, 1, norm_type=None, bias=False, act_cls=leaky_relu),
            self._conv(n_features, n_features*2, 4, 2, 1, n_extra_layers),
            self._conv(n_features*2, n_features*4, 4, 2, 1),
            self._conv(n_features*4, n_features*8, 4, 2, 1),
            ConvLayer(n_features*8, 1, 4, padding=0, norm_type=None, act_cls=None, bias=False)
        )
        self.flatten = nn.Flatten()
        self.fc = nn.Linear(10*10, 1)

    def forward(self, x):
        x = self.conv_layers(x)
        x = self.flatten(x)
        return self.fc(x)

    def _conv(self, ni, nf, ks=3, stride=2, padding=1, n_extra_layers=0):
        layers = [ConvLayer(ni, nf, ks, stride=stride, padding=padding, bias=False, norm_type=NormType.Batch, act_cls=leaky_relu)]
        for _ in range(n_extra_layers):
            layers.append(ConvLayer(nf, nf, ks, stride=1, padding=padding, bias=False, norm_type=NormType.Batch, act_cls=leaky_relu))
        return nn.Sequential(*layers)

# Función para crear un aprendiz (learner) para el crítico con fines de clasificación
def create_critic_classifier_learner(dls, metrics):
    critic = CustomCritic(in_size=224, n_channels=3, n_features=64, n_extra_layers=1)
    critic.fc = nn.Linear(critic.fc.in_features, 2)
    learn = Learner(dls, critic, metrics=metrics, loss_func=nn.CrossEntropyLoss())
    return learn

def generator_loss(fake_outputs, _=None):
    real_labels = torch.ones_like(fake_outputs, device=device)
    loss = nn.BCEWithLogitsLoss()(fake_outputs, real_labels)
    return loss

# Ejemplo de inicialización del generador condicional
z_dim = 20  # Dimensión del espacio latente
img_channels = 3  # Canales de la imagen (RGB)
features_g = 32  # Características en las capas del generador
num_classes = 2  # 'Goodware' y 'Malware'
dls = get_dls(bs=64, size=224)

# Función para generar ruido aleatorio
def get_noise(batch_size, z_dim):
    return torch.randn(batch_size, z_dim, 1, 1)

# Modificar el Learner para manejar el ruido y las etiquetas
class ConditionalGanLearner(Learner):
    def one_batch(self, i, b):
        self.iter = i
        self._split(b)
        self('before_batch')

        # Generate random noise
        batch_size = self.xb[0].shape[0]
        noise = get_noise(batch_size, z_dim).to(self.dls.device)

        # Get labels
        labels = self.yb[0].to(self.dls.device)

        # Pass noise and labels to the generator
        self.pred = self.model(noise, labels)
        self('after_pred')
        
        # Calculate the generator loss
        self.loss_grad = generator_loss(self.pred)
        self.loss = self.loss_grad.clone()

        self('after_loss')
        if not self.training or not len(self.yb): return
        self('before_backward')
        self.loss_grad.backward()

        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)  # Set max_norm to 1.0

        self._with_events(self.opt.step, 'step', CancelStepException)
        self.opt.zero_grad()
    

class CustomGenerator(nn.Module):
    def __init__(self, z_dim, n_channels, n_features=64):
        super().__init__()
        
        self.z_dim = z_dim
        self.n_features = n_features

        self.gen = nn.Sequential(
            nn.ConvTranspose2d(self.z_dim, n_features*8, 4, padding=0, bias=False),
            nn.ReLU(True),
            self._conv_transpose(n_features*8, n_features*4, 4, 2, 1),
            self._conv_transpose(n_features*4, n_features*2, 4, 2, 1),
            self._conv_transpose(n_features*2, n_features, 4, 2, 1),
            nn.ConvTranspose2d(n_features, n_channels, 4, 2, 1, bias=False),
            nn.Tanh()
        )

        self.flatten = nn.Flatten()
        self.fc = nn.Linear(10*10, 1)

    def forward(self, x):
        desired_elements = self.z_dim

        total_elements = x.nelement()

        if total_elements < desired_elements:
            repeats = desired_elements // total_elements + 1
            x = x.repeat(repeats)
            x = x[:desired_elements]
        elif total_elements > desired_elements:
            x = x[:desired_elements]

        x = x.view(-1, self.z_dim, 1, 1)
        return self.gen(x)

    def _conv_transpose(self, ni, nf, ks=4, stride=2, padding=1):
        return nn.Sequential(
            nn.ConvTranspose2d(ni, nf, ks, stride=stride, padding=padding, bias=False),
            nn.BatchNorm2d(nf),
            nn.ReLU(True)
        )

def create_generator_learner(dls, z_dim, n_channels, n_features, metrics):
    generator = CustomGenerator(z_dim, n_channels, n_features)
    learn = Learner(dls, generator, loss_func=generator_loss, metrics=metrics)
    return learn

# Create the learner
learn_gen = create_generator_learner(dls, z_dim, n_channels=3, n_features=64, metrics=[accuracy])


print("##################### Entrenando el generador ################")
# Entrenar el generador
learn_gen.fit_one_cycle(6, 1e-3)


critic_learner = create_critic_classifier_learner(dls_gen, metrics=[accuracy])
print("##################### Entrenando el crítico ################")
critic_learner.fit(6, 1e-3)

gan = GANModule(learn_gen.model, critic_learner.model)
learn_gan = Learner(dls_gen, gan, loss_func=AdaptiveLoss(nn.BCEWithLogitsLoss()), opt_func=partial(Adam, mom=0.))


print("##################### Entrenando la GAN ################")
# https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson7-superres-gan.ipynb
learn_gan.fit(40, 1e-4)

current_time = datetime.datetime.now().strftime("%Y%m%d%H%M")
learn_gan.save(f'/home/ariel.posada/data/out/gan_trained_NonObfuscatedImg_{current_time}')

critic_model = learn_gan.model.critic
critic_learner = Learner(dls_gen, critic_model, metrics=[accuracy])
preds, targs = critic_learner.get_preds(dl=dls_gen.valid)

preds_cls = preds.argmax(dim=1)
targs_cls = targs.argmax()

cm = torch.zeros(num_classes, num_classes, dtype=torch.int64)
for t, p in zip(targs_cls.view(-1), preds_cls.view(-1)):
    cm[t.long(), p.long()] += 1

plt.figure(figsize=(10, 7))
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.colorbar()
tick_marks = np.arange(num_classes)
plt.xticks(tick_marks, range(num_classes))
plt.yticks(tick_marks, range(num_classes))

# Etiquetar los ejes
plt.xlabel('Predicted')
plt.ylabel('True')

# Añadir anotaciones de texto en cada celda
thresh = cm.max() / 2.
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        plt.text(j, i, format(cm[i, j], 'd'),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

plt.tight_layout()
plt.savefig(f'/home/ariel.posada/data/out/cm_dl_{current_time}')
