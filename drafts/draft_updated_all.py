from fastai.vision.all import *
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.checkpoint import checkpoint
import argparse
import os
import math
import torchvision.transforms as transforms
from torchvision.utils import save_image
from torch.utils.data import DataLoader
from torchvision import datasets
from torch.autograd import Variable
import torch.nn.functional as F
import random
import torch.nn.parallel
import torch.backends.cudnn as cudnn
import torch.optim as optim
import torch.utils.data
import torchvision.datasets as dset
import torchvision.utils as vutils
import numpy as np
from torch.cuda.amp import GradScaler, autocast
import datetime

bs, size = 8, 224
path = "/home/ariel.posada/AFGIE1/NonObfuscatedImg"
z_dim = 100  
num_classes = 2  # 'Goodware' y 'Malware'

def get_dls(bs, size):
    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),
                       get_items=get_image_files,
                       splitter=RandomSplitter(valid_pct=0.2, seed=42),
                       get_y=parent_label,
                       item_tfms=Resize(size))
    dls = dblock.dataloaders(path, bs=bs)
    dls.c = 3
    return dls

def get_one_hot_labels(labels, num_classes):
    return F.one_hot(labels, num_classes).float()

class ConditionalGenerator(nn.Module):
    def __init__(self, z_dim, img_size, n_channels, num_classes, n_features=64):
        super().__init__()
        self.label_emb = nn.Embedding(num_classes, z_dim)
        self.z_dim = z_dim
        self.img_size = img_size
        self.num_classes = num_classes
        self.gen = nn.Sequential(
            # Inicia con un vector de tamaño z_dim
            nn.ConvTranspose2d(z_dim, n_features * 16, 4, 1, 0, bias=False),
            nn.BatchNorm2d(n_features * 16),
            nn.ReLU(True),

            # Tamaño: (n_features * 16) x 4 x 4
            nn.ConvTranspose2d(n_features * 16, n_features * 8, 4, 2, 1, bias=False),
            nn.BatchNorm2d(n_features * 8),
            nn.ReLU(True),

            # Tamaño: (n_features * 8) x 8 x 8
            nn.ConvTranspose2d(n_features * 8, n_features * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(n_features * 4),
            nn.ReLU(True),

            # Tamaño: (n_features * 4) x 16 x 16
            nn.ConvTranspose2d(n_features * 4, n_features * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(n_features * 2),
            nn.ReLU(True),

            # Tamaño: (n_features * 2) x 32 x 32
            nn.ConvTranspose2d(n_features * 2, n_features, 4, 2, 1, bias=False),
            nn.BatchNorm2d(n_features),
            nn.ReLU(True),

            # Tamaño: n_features x 64 x 64
            nn.ConvTranspose2d(n_features, n_channels, 4, 2, 1, bias=False),
            nn.Tanh()  # Tamaño final: n_channels x 128 x 128  # Tamaño final: n_channels x 128 x 128
        )

    def forward(self, noise, labels):
        # Incrustar las etiquetas y sumarlas al ruido
        label_embedding = self.label_emb(labels)
        x = torch.add(noise, label_embedding)

        # Asegurar que x tenga las dimensiones correctas
        desired_channels = 100
        current_size = x.size()

        # Verificar y ajustar las dimensiones
        if current_size[1] < desired_channels:
            # Agregar padding (cero) si hay menos canales de los necesarios
            padding = desired_channels - current_size[1]
            x = F.pad(x, (0, 0, 0, 0, 0, padding), "constant", 0)
        elif current_size[1] > desired_channels:
            # Recortar si hay más canales de los necesarios
            x = x[:, :desired_channels, :, :]

        x = self.gen(x)
        # Redimensionar x para que coincida con la entrada esperada de la primera capa convolucional transpuesta
        x = F.interpolate(x, size=(self.img_size, self.img_size), mode='bilinear', align_corners=False)
        return x

# Clase para el Discriminador
class ConditionalDiscriminator(nn.Module):
    def __init__(self, img_size, n_channels, num_classes):
        super().__init__()
        self.img_size = img_size
        self.num_classes = num_classes
        self.disc = nn.Sequential(
            # Tamaño de entrada: n_channels x 224 x 224
            nn.Conv2d(n_channels + 1, 64, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),

            # Tamaño: 64 x 112 x 112
            nn.Conv2d(64, 128, 4, 2, 1, bias=False),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),

            # Tamaño: 128 x 56 x 56
            nn.Conv2d(128, 256, 4, 2, 1, bias=False),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),

            # Tamaño: 256 x 28 x 28
            nn.Conv2d(256, 512, 4, 2, 1, bias=False),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2, inplace=True),

            # Tamaño: 512 x 14 x 14
            nn.Conv2d(512, 1, 4, 1, 0, bias=False),
            nn.Sigmoid()  # Tamaño final: 1 x 1 x 1
        )
        self.label_conditioning = nn.Embedding(num_classes, img_size * img_size)

    def forward(self, images, labels):
        label_conditioning = self.label_conditioning(labels).view(labels.size(0), 1, 224, 224)
        images = torch.cat([images, label_conditioning], 1)
        return self.disc(images)

# Función para entrenar el generador
# https://github.com/znxlwm/pytorch-generative-model-collections/issues/33
# https://github.com/otepencelik/GAN-Artwork-Generation/blob/master/cGAN.ipynb
def train_generator(dls, generator, discriminator, optimizer_g):
    scaler = GradScaler() 
    generator.train()

    total_images = len(dls.train.dataset)
    processed_images = 0

    for real_images, labels in dls.train:
        batch_size = real_images.size(0)
        noise = torch.randn(batch_size, z_dim, 1, 1)

        with autocast():
            fake_images = generator(noise, labels)
            fake_images = F.interpolate(fake_images, size=(224, 224), mode='bilinear', align_corners=False)
            disc_pred = discriminator(fake_images, labels)
            
            disc_pred = torch.sigmoid(disc_pred)  
            loss = nn.BCELoss()(disc_pred, torch.ones_like(disc_pred))  

        optimizer_g.zero_grad()

        scaler.scale(loss).backward()
        scaler.step(optimizer_g)
        scaler.update()

        processed_images += batch_size
        current_time = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        print(f"[{current_time}] Processed {processed_images}/{total_images} images")

            
# Función para entrenar el discriminador
def train_discriminator(dls, generator, discriminator, optimizer_d):
    discriminator.train()
    for real_images, labels in dls.train:
        batch_size = real_images.size(0)
        noise = torch.randn(batch_size, z_dim, 1, 1)
        fake_images = generator(noise, labels).detach()
        real_pred = discriminator(real_images, labels)
        fake_pred = discriminator(fake_images, labels)
        real_loss = nn.BCEWithLogitsLoss()(real_pred, torch.ones_like(real_pred))
        fake_loss = nn.BCEWithLogitsLoss()(fake_pred, torch.zeros_like(fake_pred))
        loss = (real_loss + fake_loss) / 2
        optimizer_d.zero_grad()
        loss.backward()
        optimizer_d.step()

# Inicialización
dls = get_dls(bs, size)
generator = ConditionalGenerator(z_dim, size, 3, num_classes)
discriminator = ConditionalDiscriminator(size, 3, num_classes)
optimizer_g = torch.optim.Adam(generator.parameters(), lr=0.0001, betas=(0.5, 0.999))
optimizer_d = torch.optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))

# Entrenamiento
for epoch in range(20):
    print(f"Epoch {epoch+1}/{20} generator.")
    train_generator(dls, generator, discriminator, optimizer_g)
    print(f"Epoch {epoch+1}/{20} discriminator.")
    train_discriminator(dls, generator, discriminator, optimizer_d)
    print(f"Epoch {epoch+1}/{20} completed.")
