
from fastai.vision.all import *
from fastai.data.transforms import *
import torch
import torch.nn as nn

import torch
import torch.nn as nn
import torch.optim as optim


from fastai.vision.all import *
import torch
import torch.nn.functional as F

import torch
from torchvision import transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

from datetime import datetime

batch_size = 64
img_height = 224
img_width = 224

path_xor = '/home/ariel.posada/AFGIE1/XORImg'
path_shikata = '/home/ariel.posada/AFGIE1/ShikataImg'

def get_combined_image_files(paths):
    all_files = []
    for path in paths:
        all_files += get_image_files(path)
    return all_files

def get_dls(bs=batch_size,size=img_height,paths=[path_xor, path_shikata]):
    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),
      get_items=lambda x: get_combined_image_files(paths),
      splitter=RandomSplitter(valid_pct=0.2,seed=42),
      get_y=parent_label,
      item_tfms=Resize(size))
    
    dls = dblock.dataloaders([path_xor , path_shikata ], bs=bs)
    dls.c = 3
    return dls

dls = get_dls()

class CGenerator(nn.Module):
    def __init__(self):
        super(CGenerator, self).__init__()
        # Concatenar ruido y etiquetas
        self.fc = nn.Sequential(
            nn.Linear(100 + 2, 7 * 7 * 512),
            nn.ReLU(True)
        )

        self.conv_trans = nn.Sequential(
            # Primer bloque de deconvolución: de 7x7 a 14x14
            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(True),
            # Segundo bloque de deconvolución: de 14x14 a 28x28
            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(True),
            # Tercer bloque de deconvolución: de 28x28 a 56x56
            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(True),
            # Cuarto bloque de deconvolución: de 56x56 a 112x112
            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(True),
            # Quinto bloque de deconvolución: de 112x112 a 224x224
            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),
            nn.Tanh()
        )


    def forward(self, noise, labels):
        x = torch.cat([noise, labels], 1)
        x = self.fc(x)
        x = x.view(x.size(0), 512, 7, 7)
        x = self.conv_trans(x)
        return x

class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.conv = nn.Sequential(
            # Primer bloque convolucional
            nn.Conv2d(1, 64, kernel_size=3, padding=1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            # Segundo bloque convolucional
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            # Tercer bloque convolucional
            nn.Conv2d(128, 256, kernel_size=3, padding=1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            # Cuarto bloque convolucional
            nn.Conv2d(256, 512, kernel_size=3, padding=1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        self.fc_common = nn.Sequential(
            nn.Linear(512 * 14 * 14, 1024),
            nn.ReLU(True),
            nn.Dropout(0.3)
        )

        # Capa final para la decisión real/falso
        self.fc_real_fake = nn.Sequential(
            nn.Linear(1024 + 2, 1),
            nn.Sigmoid()
        )

        # Capa final para la clasificación goodware/malware
        self.fc_class = nn.Sequential(
            nn.Linear(1024 + 2, 2),
            nn.Softmax(dim=1)
        )

    def forward(self, image, labels):
        x = self.conv(image)
        x = x.view(x.size(0), -1)
        x = self.fc_common(x)

        # Convertir TensorImage y TensorCategory a torch.Tensor estándar
        x = torch.tensor(x, dtype=torch.float32)
        labels = torch.tensor(labels, dtype=torch.float32)
        x_rf = torch.cat([x, labels], 1)
        x_class = torch.cat([x, labels], 1)

        real_fake_output = self.fc_real_fake(x_rf)
        class_output = self.fc_class(x_class)

        return real_fake_output, class_output

discriminator = Discriminator()
generator = CGenerator()

# Configurar el dispositivo (usar GPU si está disponible)
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
generator.to(device)
discriminator.to(device)

# Inicializar los optimizadores
gen_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))
disc_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))

def train_step(gen, disc, images, labels, gen_opt, disc_opt, loss_func):
    if images.shape[1] == 3:
        images = images.mean(dim=1, keepdim=True)

    # Generar ruido
    noise = torch.randn(images.size(0), 100).to(images.device)

    # Entrenar el generador
    gen_opt.zero_grad()
    labels_one_hot = F.one_hot(labels, num_classes=2).float()
    generated_images = gen(noise, labels_one_hot)
    fake_pred, fake_class_pred = disc(generated_images, labels_one_hot)
    gen_loss = loss_func.gen_loss(fake_pred, fake_class_pred, labels)
    gen_loss.backward()
    gen_opt.step()

    # Entrenar el discriminador
    disc_opt.zero_grad()
    real_pred, real_class_pred = disc(images, labels_one_hot)
    fake_pred, fake_class_pred = disc(generated_images.detach(), labels_one_hot)
    disc_loss = loss_func.disc_loss(real_pred, fake_pred, real_class_pred, labels)
    disc_loss.backward()
    disc_opt.step()

    return gen_loss, disc_loss

class GANLoss(nn.Module):
    def __init__(self):
        super(GANLoss, self).__init__()
        self.bce_loss = nn.BCELoss()
        self.ce_loss = nn.CrossEntropyLoss()

    def disc_loss(self, real_pred, fake_pred, real_class_pred, labels):
        # Pérdida para la clasificación real vs. falsa
        real_loss = self.bce_loss(real_pred, torch.ones_like(real_pred))
        fake_loss = self.bce_loss(fake_pred, torch.zeros_like(fake_pred))
        # Pérdida para la clasificación goodware vs. malware
        class_loss = self.ce_loss(real_class_pred, labels)
        # Combinar las pérdidas
        return real_loss + fake_loss + class_loss

    def gen_loss(self, fake_pred, fake_class_pred, labels):
        # Pérdida para engañar al discriminador
        gen_loss = self.bce_loss(fake_pred, torch.ones_like(fake_pred))
        # Pérdida para la clasificación correcta de las imágenes generadas
        class_loss = self.ce_loss(fake_class_pred, labels)
        # Combinar las pérdidas
        return gen_loss + class_loss

# Inicializar la función de pérdida
loss_func = GANLoss()


# Pre-entrenamiento de los modelos
# Inicio
def gen_loss_func(fake_pred, fake_class_pred, labels):
    bce_loss = nn.BCELoss() 
    ce_loss = nn.CrossEntropyLoss()
    gen_loss = bce_loss(fake_pred, torch.ones_like(fake_pred)) + ce_loss(fake_class_pred, labels)
    return gen_loss

def disc_loss_func(real_pred, real_class_pred, labels):
    bce_loss = nn.BCELoss()
    ce_loss = nn.CrossEntropyLoss()
    real_loss = bce_loss(real_pred, torch.ones_like(real_pred))
    disc_loss = real_loss + ce_loss(real_class_pred, labels)
    return disc_loss

def train_gen_step(gen, disc, images, labels, gen_opt, loss_func):
    if images.shape[1] == 3:
        images = images.mean(dim=1, keepdim=True)
    gen_opt.zero_grad()
    labels_one_hot = F.one_hot(labels, num_classes=2).float()
    noise = torch.randn(images.size(0), 100).to(images.device)
    generated_images = gen(noise, labels_one_hot)
    fake_pred, fake_class_pred = disc(generated_images, labels_one_hot)
    gen_loss = loss_func(fake_pred, fake_class_pred, labels)
    gen_loss.backward()
    gen_opt.step()
    return gen_loss

def train_disc_step(disc, images, labels, disc_opt, loss_func):
    if images.shape[1] == 3:
        images = images.mean(dim=1, keepdim=True)
    disc_opt.zero_grad()
    labels_one_hot = F.one_hot(labels, num_classes=2).float()
    real_pred, real_class_pred = disc(images, labels_one_hot)
    disc_loss = loss_func(real_pred, real_class_pred, labels)
    disc_loss.backward()
    disc_opt.step()
    return disc_loss


def get_predictions(discriminator, dls, batch_size):
    discriminator.eval() 
    predictions = []
    labels = []
    with torch.no_grad():  
        for images, labels_batch in dls:
            if images.shape[1] == 3:  
                images = images.mean(dim=1, keepdim=True)

            # Crear un tensor de ceros para las etiquetas
            labels_zero = torch.zeros(labels_batch.size(0), 2).float()
            _, preds = discriminator(images, labels_zero)
            preds = preds.squeeze()

            # Verificar la dimensión de las predicciones
            if preds.ndim == 1:
                preds = preds.unsqueeze(1)

            predictions.extend(preds.argmax(dim=1).cpu().numpy())
            labels.extend(labels_batch.cpu().numpy())
    return predictions, labels


EPOCHS = 60 

for epoch in range(EPOCHS):
    print(f"Discriminator Train Epoch {epoch+1}/{EPOCHS}")
    for images_batch, labels_batch in dls.train:
        disc_loss = train_disc_step(discriminator, images_batch, labels_batch, disc_optimizer, disc_loss_func)

for epoch in range(EPOCHS):
    print(f"Generator Train Epoch {epoch+1}/{EPOCHS}")
    for images_batch, labels_batch in dls.train:
        gen_loss = train_gen_step(generator, discriminator, images_batch, labels_batch, gen_optimizer, gen_loss_func)
        
# Calculate Accuracy
def calculate_accuracy(predictions, true_labels):
    correct = sum(pred == true for pred, true in zip(predictions, true_labels))
    return correct / len(predictions)

# Configuración
EPOCHS = 200 

for epoch in range(EPOCHS):
    print(f"Final Train Epoch {epoch+1}/{EPOCHS}")
    for images_batch, labels_batch in dls.train:
        if images_batch.size(0) < batch_size:
            continue
        
        images_batch = images_batch.to(device)
        labels_batch = labels_batch.to(device)

        # Entrenar un paso
        gen_loss, disc_loss = train_step(generator, discriminator, images_batch, labels_batch, gen_optimizer, disc_optimizer, loss_func)

    # Calculate Accuracy

    predictions, true_labels = get_predictions(discriminator, dls.valid, batch_size)
    accuracy = calculate_accuracy(predictions, true_labels)
    print(f"Discriminator Accuracy on Valid Dataset after Epoch {epoch+1}: {accuracy:.4f}")


predictions, true_labels = get_predictions(discriminator, dls.valid, batch_size)

cm = confusion_matrix(true_labels, predictions)

# Visualizar la matriz de confusión
plt.figure(figsize=(8, 6))
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.colorbar()
tick_marks = range(len(['Goodware', 'Malware']))
plt.xticks(tick_marks, ['Goodware', 'Malware'], rotation=45)
plt.yticks(tick_marks, ['Goodware', 'Malware'])

# Etiquetar los ejes
plt.xlabel('Predicted Label')
plt.ylabel('True Label')

# Añadir anotaciones de texto en cada celda
thresh = cm.max() / 2.
for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
    plt.text(j, i, format(cm[i, j], 'd'),
             horizontalalignment="center",
             color="white" if cm[i, j] > thresh else "black")

# Guardar la matriz de confusión como PNG
fecha_actual = datetime.now().strftime("%Y%m%d_%H%M%S")
nombre_archivo_mc = f"/home/ariel.posada/data/r18/discriminator_obfuscated_mc_{fecha_actual}.png"
plt.savefig(nombre_archivo_mc)

#nombre_archivo_modelo = f"/home/ariel.posada/data/r18/discriminator_obfuscated_{fecha_actual}.pth"
#torch.save(discriminator.state_dict(), nombre_archivo_modelo)

# /home/ariel.posada/data/discriminator_20231128_023848.pth