{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from fastai.vision.all import *\n",
    "from fastai.data.transforms import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from fastai.vision.all import *\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import timm\n",
    "\n",
    "batch_size = 64\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "\n",
    "path = \"C:\\\\Users\\\\ariel\\\\OneDrive\\\\Documents\\\\URosario\\\\MalwareClassificatorProject\\\\NonObfuscatedImg\"\n",
    "#path = \"/home/ariel.posada/AFGIE1/NonObfuscatedImg\"\n",
    "# C:\\\\Github\\\\GanObfuscatedMalwareDetection\\\\nbs\\\\output\\\\\n",
    "\n",
    "def get_dls(bs=batch_size,size=img_height,path=path):\n",
    "    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n",
    "      get_items=get_image_files,\n",
    "      splitter=RandomSplitter(valid_pct=0.33,seed=42),\n",
    "      get_y=parent_label,\n",
    "      item_tfms=Resize(size))\n",
    "    \n",
    "    dls = dblock.dataloaders(path, bs=bs)\n",
    "    dls.c = 2\n",
    "    return dls\n",
    "\n",
    "dls = get_dls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suprimir todos los warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CGenerator, self).__init__()\n",
    "        # Concatenar ruido y etiquetas\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(100 + 2, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1024, 7 * 7 * 512),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.conv_trans = nn.Sequential(\n",
    "            # Primer bloque de deconvolución: de 7x7 a 14x14\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            # Segundo bloque de deconvolución: de 14x14 a 28x28\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            # Tercer bloque de deconvolución: de 28x28 a 56x56\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            # Cuarto bloque de deconvolución: de 56x56 a 112x112\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "            # Quinto bloque de deconvolución: de 112x112 a 224x224\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            # Sexto bloque de deconvolución: de 224x224 a 224x224\n",
    "            nn.ConvTranspose2d(16, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        x = torch.cat([noise, labels], 1)\n",
    "        x = self.fc(x)\n",
    "        x = x.view(x.size(0), 512, 7, 7)\n",
    "        x = self.conv_trans(x)\n",
    "        return x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # Use timm to create a ResNet18 model, without the final classification layer\n",
    "        # Revisar si no está pre-entrenado como pasa\n",
    "        # pretrained=False\n",
    "        self.resnet18 = timm.create_model('resnet18', pretrained=False, num_classes=0)\n",
    "\n",
    "        # Fully connected layers as in your structure\n",
    "        self.fc_common = nn.Sequential(\n",
    "            nn.Linear(512, 1024),  # Adjust the input size to match ResNet18's output\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        # Capa final para la decisión real/falso\n",
    "        self.fc_real_fake = nn.Sequential(\n",
    "            nn.Linear(1024 + 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Capa final para la clasificación goodware/malware\n",
    "        self.fc_class = nn.Sequential(\n",
    "            nn.Linear(1024 + 2, 2),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, image, labels):\n",
    "        # Adjust 1-channel images to 3-channel\n",
    "        if image.shape[1] == 1:\n",
    "            image = image.repeat(1, 3, 1, 1)\n",
    "\n",
    "        # Extract features using ResNet18\n",
    "        x = self.resnet18(image)\n",
    "        \n",
    "        # Process through the fully connected layers\n",
    "        x = self.fc_common(x)\n",
    "        \n",
    "        # Convertir TensorImage y TensorCategory a torch.Tensor estándar\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.float32)\n",
    "        x_rf = torch.cat([x, labels], 1)\n",
    "        x_class = torch.cat([x, labels], 1)\n",
    "\n",
    "        real_fake_output = self.fc_real_fake(x_rf)\n",
    "        class_output = self.fc_class(x_class)\n",
    "\n",
    "        return real_fake_output, class_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "discriminator = Discriminator()\n",
    "generator = CGenerator()\n",
    "\n",
    "# Configurar el dispositivo (usar GPU si está disponible)\n",
    "device = \"cpu\" # torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "\n",
    "# Inicializar los optimizadores\n",
    "gen_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "disc_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_step(gen, disc, images, labels, gen_opt, disc_opt, loss_func):\n",
    "    if images.shape[1] == 3:\n",
    "        images = images.mean(dim=1, keepdim=True)\n",
    "\n",
    "    # Generar ruido\n",
    "    noise = torch.randn(images.size(0), 100).to(images.device)\n",
    "\n",
    "    # Entrenar el generador\n",
    "    gen_opt.zero_grad()\n",
    "    labels_one_hot = F.one_hot(labels, num_classes=2).float()\n",
    "    generated_images = gen(noise, labels_one_hot)\n",
    "    fake_pred, fake_class_pred = disc(generated_images, labels_one_hot)\n",
    "    gen_loss = loss_func.gen_loss(fake_pred, fake_class_pred, labels)\n",
    "    gen_loss.backward()\n",
    "    gen_opt.step()\n",
    "\n",
    "    # Entrenar el discriminador\n",
    "    disc_opt.zero_grad()\n",
    "    real_pred, real_class_pred = disc(images, labels_one_hot)\n",
    "    fake_pred, fake_class_pred = disc(generated_images.detach(), labels_one_hot)\n",
    "    disc_loss = loss_func.disc_loss(real_pred, fake_pred, real_class_pred, labels)\n",
    "    disc_loss.backward()\n",
    "    disc_opt.step()\n",
    "\n",
    "    return gen_loss, disc_loss\n",
    "\n",
    "class GANLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GANLoss, self).__init__()\n",
    "        self.bce_loss = nn.BCELoss()\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def disc_loss(self, real_pred, fake_pred, real_class_pred, labels):\n",
    "        # Pérdida para la clasificación real vs. falsa\n",
    "        real_loss = self.bce_loss(real_pred, torch.ones_like(real_pred))\n",
    "        fake_loss = self.bce_loss(fake_pred, torch.zeros_like(fake_pred))\n",
    "        # Pérdida para la clasificación goodware vs. malware\n",
    "        class_loss = self.ce_loss(real_class_pred, labels)\n",
    "        # Combinar las pérdidas\n",
    "        return real_loss + fake_loss + class_loss\n",
    "\n",
    "    def gen_loss(self, fake_pred, fake_class_pred, labels):\n",
    "        # Pérdida para engañar al discriminador\n",
    "        gen_loss = self.bce_loss(fake_pred, torch.ones_like(fake_pred))\n",
    "        # Pérdida para la clasificación correcta de las imágenes generadas\n",
    "        class_loss = self.ce_loss(fake_class_pred, labels)\n",
    "        # Combinar las pérdidas\n",
    "        return gen_loss + class_loss\n",
    "\n",
    "# Inicializar la función de pérdida\n",
    "loss_func = GANLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pre-entrenamiento de los modelos\n",
    "# Inicio\n",
    "def gen_loss_func(fake_pred, fake_class_pred, labels):\n",
    "    bce_loss = nn.BCELoss() \n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    gen_loss = bce_loss(fake_pred, torch.ones_like(fake_pred)) + ce_loss(fake_class_pred, labels)\n",
    "    return gen_loss\n",
    "\n",
    "def disc_loss_func(real_pred, real_class_pred, labels):\n",
    "    bce_loss = nn.BCELoss()\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    real_loss = bce_loss(real_pred, torch.ones_like(real_pred))\n",
    "    disc_loss = real_loss + ce_loss(real_class_pred, labels)\n",
    "    return disc_loss\n",
    "\n",
    "def train_gen_step(gen, disc, images, labels, gen_opt, loss_func):\n",
    "    if images.shape[1] == 3:\n",
    "        images = images.mean(dim=1, keepdim=True)\n",
    "    gen_opt.zero_grad()\n",
    "    labels_one_hot = F.one_hot(labels, num_classes=2).float()\n",
    "    noise = torch.randn(images.size(0), 100).to(images.device)\n",
    "    generated_images = gen(noise, labels_one_hot)\n",
    "    fake_pred, fake_class_pred = disc(generated_images, labels_one_hot)\n",
    "    gen_loss = loss_func(fake_pred, fake_class_pred, labels)\n",
    "    gen_loss.backward()\n",
    "    gen_opt.step()\n",
    "    return gen_loss\n",
    "\n",
    "def train_disc_step(disc, images, labels, disc_opt, loss_func):\n",
    "    if images.shape[1] == 3:\n",
    "        images = images.mean(dim=1, keepdim=True)\n",
    "    disc_opt.zero_grad()\n",
    "    labels_one_hot = F.one_hot(labels, num_classes=2).float()\n",
    "    real_pred, real_class_pred = disc(images, labels_one_hot)\n",
    "    disc_loss = loss_func(real_pred, real_class_pred, labels)\n",
    "    disc_loss.backward()\n",
    "    disc_opt.step()\n",
    "    return disc_loss\n",
    "\n",
    "\n",
    "def get_predictions(discriminator, dls, batch_size):\n",
    "    discriminator.eval() \n",
    "    predictions = []\n",
    "    labels = []\n",
    "    with torch.no_grad():  \n",
    "        for images, labels_batch in dls:\n",
    "            if images.shape[1] == 3:  \n",
    "                images = images.mean(dim=1, keepdim=True)\n",
    "\n",
    "            # Crear un tensor de ceros para las etiquetas\n",
    "            labels_zero = torch.zeros(labels_batch.size(0), 2).float()\n",
    "            _, preds = discriminator(images, labels_zero)\n",
    "            preds = preds.squeeze()\n",
    "\n",
    "            # Verificar la dimensión de las predicciones\n",
    "            if preds.ndim == 1:\n",
    "                preds = preds.unsqueeze(1)\n",
    "\n",
    "            predictions.extend(preds.argmax(dim=1).cpu().numpy())\n",
    "            labels.extend(labels_batch.cpu().numpy())\n",
    "    return predictions, labels\n",
    "\n",
    "# Calculate Accuracy\n",
    "def calculate_accuracy(predictions, true_labels):\n",
    "    correct = sum(pred == true for pred, true in zip(predictions, true_labels))\n",
    "    return correct / len(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator Resnet18 Train Epoch 1/20\n",
      "Discriminator Accuracy on Valid Dataset after Epoch 1: 0.8731\n",
      "Discriminator Resnet18 Train Epoch 2/20\n",
      "Discriminator Accuracy on Valid Dataset after Epoch 2: 0.8901\n",
      "Discriminator Resnet18 Train Epoch 3/20\n",
      "Discriminator Accuracy on Valid Dataset after Epoch 3: 0.9032\n",
      "Discriminator Resnet18 Train Epoch 4/20\n",
      "Discriminator Accuracy on Valid Dataset after Epoch 4: 0.9077\n",
      "Discriminator Resnet18 Train Epoch 5/20\n",
      "Discriminator Accuracy on Valid Dataset after Epoch 5: 0.9323\n",
      "Discriminator Resnet18 Train Epoch 6/20\n",
      "Discriminator Accuracy on Valid Dataset after Epoch 6: 0.9336\n",
      "Discriminator Resnet18 Train Epoch 7/20\n",
      "Discriminator Accuracy on Valid Dataset after Epoch 7: 0.9375\n",
      "Discriminator Resnet18 Train Epoch 8/20\n",
      "Discriminator Accuracy on Valid Dataset after Epoch 8: 0.9365\n",
      "Discriminator Resnet18 Train Epoch 9/20\n",
      "Discriminator Accuracy on Valid Dataset after Epoch 9: 0.9352\n",
      "Discriminator Resnet18 Train Epoch 10/20\n",
      "Discriminator Accuracy on Valid Dataset after Epoch 10: 0.9368\n",
      "Discriminator Resnet18 Train Epoch 11/20\n",
      "Discriminator Accuracy on Valid Dataset after Epoch 11: 0.9614\n",
      "Discriminator Resnet18 Train Epoch 12/20\n",
      "Discriminator Accuracy on Valid Dataset after Epoch 12: 0.9588\n",
      "Discriminator Resnet18 Train Epoch 13/20\n",
      "Discriminator Accuracy on Valid Dataset after Epoch 13: 0.9616\n",
      "Discriminator Resnet18 Train Epoch 14/20\n",
      "Discriminator Accuracy on Valid Dataset after Epoch 14: 0.9577\n",
      "Discriminator Resnet18 Train Epoch 15/20\n",
      "Discriminator Accuracy on Valid Dataset after Epoch 15: 0.9611\n",
      "Discriminator Resnet18 Train Epoch 16/20\n",
      "Discriminator Accuracy on Valid Dataset after Epoch 16: 0.9569\n",
      "Discriminator Resnet18 Train Epoch 17/20\n",
      "Discriminator Accuracy on Valid Dataset after Epoch 17: 0.9612\n",
      "Discriminator Resnet18 Train Epoch 18/20\n",
      "Discriminator Accuracy on Valid Dataset after Epoch 18: 0.9634\n",
      "Discriminator Resnet18 Train Epoch 19/20\n",
      "Discriminator Accuracy on Valid Dataset after Epoch 19: 0.9577\n",
      "Discriminator Resnet18 Train Epoch 20/20\n",
      "Discriminator Accuracy on Valid Dataset after Epoch 20: 0.9630\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# EPOCHS = 60 \n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Discriminator Resnet18 Train Epoch {epoch+1}/{EPOCHS}\")\n",
    "    for images_batch, labels_batch in dls.train:\n",
    "        disc_loss = train_disc_step(discriminator, images_batch, labels_batch, disc_optimizer, disc_loss_func)\n",
    "    predictions, true_labels = get_predictions(discriminator, dls.valid, batch_size)\n",
    "    accuracy = calculate_accuracy(predictions, true_labels)\n",
    "    print(f\"Discriminator Accuracy on Valid Dataset after Epoch {epoch+1}: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Save the discriminator\n",
    "    \n",
    "fecha_actual_pre_train = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "nombre_archivo_mc_pre_gan = f\"C:\\\\Github\\\\GanObfuscatedMalwareDetection\\\\nbs\\\\output\\\\discriminator_pre_gan_{fecha_actual_pre_train}.pth\"\n",
    "torch.save(discriminator.state_dict(), f\"{nombre_archivo_mc_pre_gan}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator Train Epoch 1/20\n",
      "Generator Train Epoch 2/20\n",
      "Generator Train Epoch 3/20\n",
      "Generator Train Epoch 4/20\n",
      "Generator Train Epoch 5/20\n",
      "Generator Train Epoch 6/20\n",
      "Generator Train Epoch 7/20\n",
      "Generator Train Epoch 8/20\n",
      "Generator Train Epoch 9/20\n",
      "Generator Train Epoch 10/20\n",
      "Generator Train Epoch 11/20\n",
      "Generator Train Epoch 12/20\n",
      "Generator Train Epoch 13/20\n",
      "Generator Train Epoch 14/20\n",
      "Generator Train Epoch 15/20\n",
      "Generator Train Epoch 16/20\n",
      "Generator Train Epoch 17/20\n",
      "Generator Train Epoch 18/20\n",
      "Generator Train Epoch 19/20\n",
      "Generator Train Epoch 20/20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Generator Train Epoch {epoch+1}/{EPOCHS}\")\n",
    "    for images_batch, labels_batch in dls.train:\n",
    "        gen_loss = train_gen_step(generator, discriminator, images_batch, labels_batch, gen_optimizer, gen_loss_func)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Train Resnet18 Epoch 1/20\n",
      "Discriminator Accuracy on Valid Dataset after Epoch 1: 0.9642\n",
      "Final Train Resnet18 Epoch 2/20\n",
      "Discriminator Accuracy on Valid Dataset after Epoch 2: 0.9624\n",
      "Final Train Resnet18 Epoch 3/20\n",
      "Discriminator Accuracy on Valid Dataset after Epoch 3: 0.9625\n",
      "Final Train Resnet18 Epoch 4/20\n",
      "Discriminator Accuracy on Valid Dataset after Epoch 4: 0.9654\n",
      "Final Train Resnet18 Epoch 5/20\n",
      "Discriminator Accuracy on Valid Dataset after Epoch 5: 0.9664\n",
      "Final Train Resnet18 Epoch 6/20\n",
      "Discriminator Accuracy on Valid Dataset after Epoch 6: 0.9623\n",
      "Final Train Resnet18 Epoch 7/20\n",
      "Discriminator Accuracy on Valid Dataset after Epoch 7: 0.9647\n",
      "Final Train Resnet18 Epoch 8/20\n",
      "Discriminator Accuracy on Valid Dataset after Epoch 8: 0.9623\n",
      "Final Train Resnet18 Epoch 9/20\n",
      "Discriminator Accuracy on Valid Dataset after Epoch 9: 0.9649\n",
      "Final Train Resnet18 Epoch 10/20\n",
      "Discriminator Accuracy on Valid Dataset after Epoch 10: 0.9632\n",
      "Final Train Resnet18 Epoch 11/20\n",
      "Discriminator Accuracy on Valid Dataset after Epoch 11: 0.9641\n",
      "Final Train Resnet18 Epoch 12/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m     labels_batch \u001b[38;5;241m=\u001b[39m labels_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# Entrenar un paso\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m     gen_loss, disc_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisc_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Calculate Accuracy\u001b[39;00m\n\u001b[0;32m     19\u001b[0m predictions, true_labels \u001b[38;5;241m=\u001b[39m get_predictions(discriminator, dls\u001b[38;5;241m.\u001b[39mvalid, batch_size)\n",
      "Cell \u001b[1;32mIn[15], line 19\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(gen, disc, images, labels, gen_opt, disc_opt, loss_func)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Entrenar el discriminador\u001b[39;00m\n\u001b[0;32m     18\u001b[0m disc_opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 19\u001b[0m real_pred, real_class_pred \u001b[38;5;241m=\u001b[39m \u001b[43mdisc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_one_hot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m fake_pred, fake_class_pred \u001b[38;5;241m=\u001b[39m disc(generated_images\u001b[38;5;241m.\u001b[39mdetach(), labels_one_hot)\n\u001b[0;32m     21\u001b[0m disc_loss \u001b[38;5;241m=\u001b[39m loss_func\u001b[38;5;241m.\u001b[39mdisc_loss(real_pred, fake_pred, real_class_pred, labels)\n",
      "File \u001b[1;32mc:\\Users\\ariel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ariel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 82\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[1;34m(self, image, labels)\u001b[0m\n\u001b[0;32m     79\u001b[0m     image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Extract features using ResNet18\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresnet18\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# Process through the fully connected layers\u001b[39;00m\n\u001b[0;32m     85\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_common(x)\n",
      "File \u001b[1;32mc:\\Users\\ariel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ariel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ariel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\timm\\models\\resnet.py:578\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 578\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    579\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_head(x)\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\ariel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\timm\\models\\resnet.py:565\u001b[0m, in \u001b[0;36mResNet.forward_features\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    563\u001b[0m     x \u001b[38;5;241m=\u001b[39m checkpoint_seq([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4], x, flatten\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 565\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    566\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[0;32m    567\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n",
      "File \u001b[1;32mc:\\Users\\ariel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ariel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ariel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ariel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ariel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ariel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\timm\\models\\resnet.py:117\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    115\u001b[0m     shortcut \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m--> 117\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[0;32m    119\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_block(x)\n",
      "File \u001b[1;32mc:\\Users\\ariel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ariel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ariel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ariel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ariel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\fastai\\torch_core.py:382\u001b[0m, in \u001b[0;36mTensorBase.__torch_function__\u001b[1;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mdebug \u001b[38;5;129;01mand\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__str__\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__repr__\u001b[39m\u001b[38;5;124m'\u001b[39m): \u001b[38;5;28mprint\u001b[39m(func, types, args, kwargs)\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _torch_handled(args, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_opt, func): types \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mTensor,)\n\u001b[1;32m--> 382\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mifnone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    383\u001b[0m dict_objs \u001b[38;5;241m=\u001b[39m _find_args(args) \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;28;01melse\u001b[39;00m _find_args(\u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mvalues()))\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mtype\u001b[39m(res),TensorBase) \u001b[38;5;129;01mand\u001b[39;00m dict_objs: res\u001b[38;5;241m.\u001b[39mset_meta(dict_objs[\u001b[38;5;241m0\u001b[39m],as_copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\ariel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:1418\u001b[0m, in \u001b[0;36mTensor.__torch_function__\u001b[1;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m   1415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m   1417\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[1;32m-> 1418\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1419\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m get_default_nowrap_functions():\n\u001b[0;32m   1420\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Configuración\n",
    "# EPOCHS = 80 \n",
    "EPOCHS = 20\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Final Train Resnet18 Epoch {epoch+1}/{EPOCHS}\")\n",
    "    for images_batch, labels_batch in dls.train:\n",
    "        if images_batch.size(0) < batch_size:\n",
    "            continue\n",
    "        \n",
    "        images_batch = images_batch.to(device)\n",
    "        labels_batch = labels_batch.to(device)\n",
    "\n",
    "        # Entrenar un paso\n",
    "        gen_loss, disc_loss = train_step(generator, discriminator, images_batch, labels_batch, gen_optimizer, disc_optimizer, loss_func)\n",
    "\n",
    "    # Calculate Accuracy\n",
    "    predictions, true_labels = get_predictions(discriminator, dls.valid, batch_size)\n",
    "    accuracy = calculate_accuracy(predictions, true_labels)\n",
    "    print(f\"Discriminator Accuracy on Valid Dataset after Epoch {epoch+1}: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions, true_labels = get_predictions(discriminator, dls.valid, batch_size)\n",
    "\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "# Visualizar la matriz de confusión\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = range(len(['Goodware', 'Malware']))\n",
    "plt.xticks(tick_marks, ['Goodware', 'Malware'], rotation=45)\n",
    "plt.yticks(tick_marks, ['Goodware', 'Malware'])\n",
    "\n",
    "# Etiquetar los ejes\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Añadir anotaciones de texto en cada celda\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm[i, j], 'd'),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "# Guardar la matriz de confusión como PNG\n",
    "fecha_actual = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "nombre_archivo_mc = f\"C:\\\\Github\\\\GanObfuscatedMalwareDetection\\\\nbs\\\\output\\\\discriminator_Resnet18_mc_{fecha_actual}.png\"\n",
    "plt.savefig(nombre_archivo_mc)\n",
    "\n",
    "nombre_archivo_modelo = f\"C:\\\\Github\\\\GanObfuscatedMalwareDetection\\\\nbs\\\\output\\\\discriminator_Resnet18_{fecha_actual}.pth\"\n",
    "torch.save(discriminator.state_dict(), nombre_archivo_modelo)\n",
    "\n",
    "nombre_archivo_modelo = f\"C:\\\\Github\\\\GanObfuscatedMalwareDetection\\\\nbs\\\\output\\\\generator_Resnet18_{fecha_actual}.pth\"\n",
    "torch.save(generator.state_dict(), nombre_archivo_modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# /home/ariel.posada/data/discriminator_20231128_023848.pth\n",
    "\n",
    "# Reset the Discriminator\n",
    "discriminator.load_state_dict(torch.load(nombre_archivo_mc_pre_gan))\n",
    "\n",
    "# Train it again with normal training\n",
    "EPOCHS = 20\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Discriminator Resnet18 Train Epoch {epoch+1}/{EPOCHS}\")\n",
    "    for images_batch, labels_batch in dls.train:\n",
    "        disc_loss = train_disc_step(discriminator, images_batch, labels_batch, disc_optimizer, disc_loss_func)\n",
    "    predictions, true_labels = get_predictions(discriminator, dls.valid, batch_size)\n",
    "    accuracy = calculate_accuracy(predictions, true_labels)\n",
    "    print(f\"Discriminator Accuracy on Valid Dataset after Epoch {epoch+1}: {accuracy:.4f}\")\n",
    "    print(f\"Real Epoch: {epoch+1 + 10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions, true_labels = get_predictions(discriminator, dls.valid, batch_size)\n",
    "\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "# Visualizar la matriz de confusión\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = range(len(['Goodware', 'Malware']))\n",
    "plt.xticks(tick_marks, ['Goodware', 'Malware'], rotation=45)\n",
    "plt.yticks(tick_marks, ['Goodware', 'Malware'])\n",
    "\n",
    "# Etiquetar los ejes\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Añadir anotaciones de texto en cada celda\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm[i, j], 'd'),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "# Guardar la matriz de confusión como PNG\n",
    "fecha_actual = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "nombre_archivo_mc = f\"C:\\\\Github\\\\GanObfuscatedMalwareDetection\\\\nbs\\\\output\\\\discriminator_Resnet18_nogan_{fecha_actual}.png\"\n",
    "plt.savefig(nombre_archivo_mc)\n",
    "\n",
    "nombre_archivo_modelo = f\"C:\\\\Github\\\\GanObfuscatedMalwareDetection\\\\nbs\\\\output\\\\discriminator_Resnet18_nogan_{fecha_actual}.pth\"\n",
    "torch.save(discriminator.state_dict(), nombre_archivo_modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tensor_to_image(tensor, file_path=None):\n",
    "    tensor = tensor.detach().cpu()\n",
    "    img = transforms.ToPILImage()(tensor)\n",
    "\n",
    "    if file_path is not None:\n",
    "        img.save(file_path)\n",
    "    \n",
    "    return img\n",
    "\n",
    "print(f\"Generating malware images...\")\n",
    "\n",
    "# Generate 20 images of Malware\n",
    "for i in range(20):\n",
    "    noise = torch.randn(1, 100).to(device)\n",
    "    # Goodware = 0, Malware = 1\n",
    "    labels_one_hot = torch.tensor([[0, 1]]).float().to(device)\n",
    "\n",
    "    # Generate an image\n",
    "    img_tensor = generator(noise, labels_one_hot)\n",
    "    img = tensor_to_image(img_tensor[0])\n",
    "    # Display the image\n",
    "    tensor_to_image(img_tensor[0], f'C:\\\\Github\\\\GanObfuscatedMalwareDetection\\\\nbs\\\\output\\\\img_Malware_{i+1}.png')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Generating goodware images...\")\n",
    "# Generate 20 images of Goodware\n",
    "for i in range(20):\n",
    "    noise = torch.randn(1, 100).to(device)\n",
    "    # Goodware = 1, Malware = 0\n",
    "    labels_one_hot = torch.tensor([[1, 0]]).float().to(device)\n",
    "\n",
    "    # Generate an image\n",
    "    img_tensor = generator(noise, labels_one_hot)\n",
    "    img = tensor_to_image(img_tensor[0])\n",
    "    # Display the image\n",
    "    tensor_to_image(img_tensor[0], f'C:\\\\Github\\\\GanObfuscatedMalwareDetection\\\\nbs\\\\output\\\\img_Goodware_{i+1}.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
